{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c71812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Increasing the columns views limit\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 150\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dd0a4",
   "metadata": {},
   "source": [
    "## Reading and Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baba60a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Leads.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Reading the data file using pandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeads.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Leads.csv'"
     ]
    }
   ],
   "source": [
    "#Reading the data file using pandas\n",
    "df = pd.read_csv('Leads.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether there are any duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets have a look at all the columns, their datatypes and also get an idea of null values present\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42150c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d59bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change nomenclature to snakecase\n",
    "df.columns = df.columns.str.replace(' ', '_').str.lower()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten column names\n",
    "df.rename(columns = {'totalvisits': 'total_visits', 'total_time_spent_on_website': 'time_on_website', \n",
    "                    'how_did_you_hear_about_x_education': 'source', 'what_is_your_current_occupation': 'occupation',\n",
    "                    'what_matters_most_to_you_in_choosing_a_course' : 'course_selection_reason', \n",
    "                    'receive_more_updates_about_our_courses': 'courses_updates', \n",
    "                     'update_me_on_supply_chain_content': 'supply_chain_content_updates',\n",
    "                    'get_updates_on_dm_content': 'dm_content_updates',\n",
    "                    'i_agree_to_pay_the_amount_through_cheque': 'cheque_payment',\n",
    "                    'a_free_copy_of_mastering_the_interview': 'mastering_interview'}, inplace = True)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcd4d02",
   "metadata": {},
   "source": [
    "### Drop `prospect_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('prospect_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66bfad",
   "metadata": {},
   "source": [
    "### Replace \"Select\" category with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b7656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all non-numeric columns\n",
    "df_obj = df.select_dtypes(include='object')\n",
    "\n",
    "# Find out columns that have \"Select\"\n",
    "s = lambda x: x.str.contains('Select', na=False)\n",
    "l = df_obj.columns[df_obj.apply(s).any()].tolist()\n",
    "print (l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de825d77",
   "metadata": {},
   "source": [
    "There are 4 columns that contains `Select`, which are effectively null values. We are going to make that change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09138f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the columns that have a \"Select\" entry\n",
    "sel_cols = ['specialization', 'source', 'lead_profile', 'city']\n",
    "\n",
    "# replace values\n",
    "df[sel_cols] = df[sel_cols].replace('Select', np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d686a4d",
   "metadata": {},
   "source": [
    "### Handle null values and sales generated columns\n",
    "\n",
    "- Given there are a number of columns with very high number of null entries, let's calculate the percentage of null values in each column, and take a decision from there.\n",
    "- Furthermore, we can also drop Sales generated columns because those are the data entries that are made after the sales team has connected with the student. Those data have no bearing to the purpose of our model ie. providing lead score. The columns are\n",
    "    * `tags`\n",
    "    * `lead_quality`\n",
    "    * all `asymmetrique` columns\n",
    "    * `last_activity`\n",
    "    * `last_notable_activity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6973fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of null values for each column\n",
    "(df.isnull().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce04d1f9",
   "metadata": {},
   "source": [
    "#### Drop columns that have null values > 40% or Sales generated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cff6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['source', 'lead_quality', 'lead_profile', 'asymmetrique_activity_index', \n",
    "                      'asymmetrique_profile_index', 'asymmetrique_activity_score', 'asymmetrique_profile_score',\n",
    "        'tags', 'last_activity', 'last_notable_activity'], \n",
    "        axis = 1, inplace = True)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at what are we left with\n",
    "# Calculate percentage of null values for each column\n",
    "(df.isnull().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f656b5",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "<br>There are five columns that still have high null values: `country`, `specialization`, `occupation`, `course_selection_reason`, and `city`. We will look at them individually to see what can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30938c70",
   "metadata": {},
   "source": [
    "#### `country` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eafe69",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "<br> The distribution of the data is very heavily skewed, with India + null values = 97% of the total. It is safe to drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('country', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387877e",
   "metadata": {},
   "source": [
    "#### `course_selection_reason` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.course_selection_reason.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcafca3",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "<br> The distribution of the data is very heavily skewed, with Better career prospects + null values = approx 100% of the total. It is safe to drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('course_selection_reason', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239e414",
   "metadata": {},
   "source": [
    "#### `occupation` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.occupation.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9f151",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "<br> For occupation, we can first combine categories, and then impute proportionally to maintain the distribution and not introduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine low representing categories\n",
    "df.loc[(df.occupation == 'Student') | (df.occupation == 'Other') | (df.occupation == 'Housewife') | \n",
    "       (df.occupation == 'Businessman') , 'occupation'] = 'Student and Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f837a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.occupation.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute proportionately\n",
    "df['occupation'] = df.occupation.fillna(pd.Series(np.random.choice(['Unemployed', 'Working Professional', 'Student and Others'], p = [0.8550, 0.1078, 0.0372], size = len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d559bd2",
   "metadata": {},
   "source": [
    "#### `specialization` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.specialization.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885ffe1",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "<br> For specialization, we can first combine categories based on the course type, and then impute proportionally to maintain the distribution and not introduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize all management courses\n",
    "df.loc[(df.specialization == 'Finance Management') | (df.specialization == 'Human Resource Management') | \n",
    "       (df.specialization == 'Marketing Management') |  (df.specialization == 'Operations Management') |\n",
    "       (df.specialization == 'IT Projects Management') | (df.specialization == 'Supply Chain Management') |\n",
    "       (df.specialization == 'Healthcare Management') | (df.specialization == 'Hospitality Management') |\n",
    "       (df.specialization == 'Retail Management') , 'specialization'] = 'Management Specializations'\n",
    "\n",
    "# categorize all busines courses\n",
    "df.loc[(df.specialization == 'Business Administration') | (df.specialization == 'International Business') | \n",
    "       (df.specialization == 'Rural and Agribusiness') | (df.specialization == 'E-Business') \n",
    "        , 'specialization'] = 'Business Specializations'\n",
    "\n",
    "# categorize all industry courses\n",
    "df.loc[(df.specialization == 'Banking, Investment And Insurance') | (df.specialization == 'Media and Advertising') |\n",
    "       (df.specialization == 'Travel and Tourism') | (df.specialization == 'Services Excellence') |\n",
    "       (df.specialization == 'E-COMMERCE'), 'specialization'] = 'Industry Specializations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fcb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.specialization.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute proportionately\n",
    "df['specialization'] = df.specialization.fillna(pd.Series(np.random.choice(['Management Specializations', 'Business Specializations', 'Industry Specializations'], p = [0.7258, 0.1213, 0.1529 ], size = len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba5fb1",
   "metadata": {},
   "source": [
    "#### `city` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.city.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21044a11",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "We will categorize cities based on logical decisions and impute proportionately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize all non-mumbai, but Maharashtra cities\n",
    "df.loc[(df.city == 'Thane & Outskirts') | (df.city == 'Other Cities of Maharashtra'), 'city'] = 'Non-Mumbai Maharashtra Cities'\n",
    "\n",
    "# categorize all other cities\n",
    "df.loc[(df.city == 'Other Cities') | (df.city == 'Other Metro Cities') | (df.city == 'Tier II Cities') , 'city'] = 'Non-Maharashtra Cities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5930ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.city.value_counts(normalize = True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute proportionately\n",
    "df['city'] = df.city.fillna(pd.Series(np.random.choice(['Mumbai', 'Non-Mumbai Maharashtra Cities','Non-Maharashtra Cities'], p = [0.5784, 0.2170, 0.2046 ], size = len(df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6e4ba",
   "metadata": {},
   "source": [
    "### Handle categorical columns with low number of missing values and low representation of categories\n",
    "\n",
    "In this step, we will go through the rest of the categorical columns one by one and\n",
    "* Merge categories that have low representation\n",
    "* Impute the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae47986",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isnull().sum() / df.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine unique values for all object datatype columns\n",
    "for k, v in df.select_dtypes(include='object').nunique().to_dict().items():\n",
    "    print('{} = {}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44724696",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "<br> As can be seen from the above output, the categorical columns (i.e. number of unique values > 2) are:\n",
    "* `lead_origin`\n",
    "* `lead_source`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548e4da",
   "metadata": {},
   "source": [
    "#### `lead_origin` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74561987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lead_origin.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24593d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a lot of smaller values which will not be used as definitive factors, lets group them together\n",
    "df.loc[(df.lead_origin == 'Lead Import') | (df.lead_origin == 'Quick Add Form') | (df.lead_origin == 'Lead Add Form'), 'lead_origin'] = 'Lead Add Form and Others'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c124e3",
   "metadata": {},
   "source": [
    "#### `lead_source` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lead_source.value_counts(normalize = True, dropna = False) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets impute the missing values with the mode of data i.e. clearly 'Google'\n",
    "df.lead_source.fillna(df.lead_source.mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a lot of smaller values which will not be used as definitive factors, lets group them together\n",
    "df['lead_source'] = df['lead_source'].apply(lambda x: x if ((x== 'Google') | (x=='Direct Traffic') | (x=='Olark Chat') |  (x=='Organic Search') | (x=='Reference'))else 'Other Social Sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d3ce3",
   "metadata": {},
   "source": [
    "### Handle Binary columns\n",
    "\n",
    "* Drop those columns that have significant data imbalance\n",
    "* Drop all those columns that have only 1 unique entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d26435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine unique values\n",
    "for k, v in df.select_dtypes(include='object').nunique().to_dict().items():\n",
    "    print('{} = {}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff3eba0",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "* The following columns can be dropped as they have just 1 unique values\n",
    "    * `magazine`\n",
    "    * `course_updates`\n",
    "    * `supply_chain_content_updates`\n",
    "    * `dm_content_updates`\n",
    "    * `cheque_payment`\n",
    "    \n",
    " Let's now check the data imbalance for the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rest of the binary columns in a new dataframe\n",
    "df_bin = df[['do_not_email', 'do_not_call', 'search', 'newspaper_article', 'x_education_forums', \n",
    "           'newspaper', 'digital_advertisement', 'through_recommendations', 'mastering_interview']]\n",
    "\n",
    "# see value counts for each of the columns\n",
    "for i in df_bin.columns:\n",
    "    x = (df_bin[i].value_counts(normalize = True)) * 100\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c3099",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "<br> Because of heavy data imbalance, we can drop the following columns as well\n",
    "* `do_not_call`\n",
    "* `search`\n",
    "* `newspaper_article`\n",
    "* `x_education_forums`\n",
    "* `newspaper`\n",
    "* `digital_advertisement`\n",
    "* `through_recommendations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_bin = ['do_not_call', 'search', 'newspaper_article', 'x_education_forums', \n",
    "           'newspaper', 'digital_advertisement', 'through_recommendations', 'magazine', 'courses_updates', \n",
    "           'supply_chain_content_updates', 'dm_content_updates', 'cheque_payment']\n",
    "\n",
    "df.drop(drop_bin, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba383bc",
   "metadata": {},
   "source": [
    "### Handle Numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98961bf8",
   "metadata": {},
   "source": [
    "#### `lead_number` column: change datatype\n",
    "\n",
    "`lead_number` column is a unique identifier for each leads. Therefore, aggregations won't be of any relevance. We should change it to object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee286a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lead_number = df.lead_number.astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9711e6",
   "metadata": {},
   "source": [
    "#### `total_visits` column\n",
    "\n",
    "For this column, we need to handle the missing values, and can convert the datatype to integer since visits can't be decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b80628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_visits.fillna(df.total_visits.median(), inplace=True)\n",
    "df.total_visits = df.total_visits.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed832e",
   "metadata": {},
   "source": [
    "#### `page_views_per_visit` column\n",
    "\n",
    "Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ea9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.page_views_per_visit.fillna(df.page_views_per_visit.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f3f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13ac27",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab4acc",
   "metadata": {},
   "source": [
    "### Numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# See distribution of each of these columns\n",
    "fig = plt.figure(figsize = (14, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(df.total_visits, bins = 20)\n",
    "plt.title('Total website visits')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(df.time_on_website, bins = 20)\n",
    "plt.title('Time spent on website')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(df.page_views_per_visit, bins = 20)\n",
    "plt.title('Average number of page views per visit')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6729f",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- High peaks and skewed data. There might be a possibility of outliers. We will check them next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983af6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,12))\n",
    "sns.heatmap(df[['total_visits', 'time_on_website', 'page_views_per_visit']].corr(), cmap=\"YlGnBu\", annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fa973",
   "metadata": {},
   "source": [
    "**Observations**: No significaqnt correlation such that columns can be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b85f5",
   "metadata": {},
   "source": [
    "#### Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 14))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "sns.boxplot(df.total_visits)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "sns.boxplot(df.time_on_website)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "sns.boxplot(df.page_views_per_visit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd9139",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "* Looking at both the box plots and the statistics, there are upper bound outliers in both `total_visits` and `page_views_per_visit` columns. We can also see that the data can be capped at 99 percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec13e2",
   "metadata": {},
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41f784",
   "metadata": {},
   "source": [
    "#### Lead Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "df.groupby('lead_origin')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8, \n",
    "                                                            edgecolor = 'black', \n",
    "                                                            color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc47e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da4fe5",
   "metadata": {},
   "source": [
    "#### Lead Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "df.groupby('lead_source')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8, \n",
    "                                                            edgecolor = 'black', \n",
    "                                                            color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940aac3d",
   "metadata": {},
   "source": [
    "#### Specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "df.groupby('specialization')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8, \n",
    "                                                            edgecolor = 'black', \n",
    "                                                            color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be3449",
   "metadata": {},
   "source": [
    "Most of the speciliazation taken are management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7e0ba",
   "metadata": {},
   "source": [
    "#### Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "df.groupby('occupation')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8,edgecolor = 'black', color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed41e4b",
   "metadata": {},
   "source": [
    "Unempployed users are the most significant leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c52177",
   "metadata": {},
   "source": [
    "#### City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "df.groupby('city')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8, \n",
    "                                                            edgecolor = 'black', \n",
    "                                                            color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a7b50",
   "metadata": {},
   "source": [
    "Mumbai in particular and Maharashtra in general dominates the lead. This is likely due to the fact that the courses are based in Mumbai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 8))\n",
    "\n",
    "df.groupby('do_not_email')['lead_number'].count().sort_values(ascending = False).plot(kind= 'barh', width = 0.8, \n",
    "                                                            edgecolor = 'black', \n",
    "                                                            color = plt.cm.Paired(np.arange(len(df))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b92cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9885206",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec981070",
   "metadata": {},
   "source": [
    "### Converting Binary (Yes/No) to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72898696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine unique values\n",
    "for k, v in df.select_dtypes(include='object').nunique().to_dict().items():\n",
    "    print('{} = {}'.format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9ff10",
   "metadata": {},
   "source": [
    "We have two binary columns: `do_not_email`, `mastering_interview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "binlist = ['do_not_email', 'mastering_interview']\n",
    "\n",
    "# Defining the map function\n",
    "def binary_map(x):\n",
    "    return x.map({'Yes': 1, \"No\": 0})\n",
    "\n",
    "# Applying the function to the housing list\n",
    "df[binlist] = df[binlist].apply(binary_map)\n",
    "\n",
    "# check the operation was success\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12131cbc",
   "metadata": {},
   "source": [
    "### Creating dummy variable for categorical columns\n",
    "\n",
    "Categorical columns are: `lead_origin`, `lead_source`, `specialization`, `occupation`, `city`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dummy variable for some of the categorical variables and dropping the first one.\n",
    "dummy1 = pd.get_dummies(df[['lead_origin', 'lead_source', 'specialization', 'occupation', 'city']], drop_first = True)\n",
    "\n",
    "# Adding the results to the master dataframe\n",
    "df = pd.concat([df, dummy1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns for which dummies have been created\n",
    "df.drop(['lead_origin', 'lead_source', 'specialization', 'occupation', 'city'], axis = 1, inplace = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7d30d",
   "metadata": {},
   "source": [
    "### Outliers Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d612ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df[['total_visits', 'time_on_website', 'page_views_per_visit']]\n",
    "\n",
    "# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\n",
    "num_cols.describe(percentiles=[.25, .5, .75, .90, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capping at 99 percentile\n",
    "df.total_visits.loc[df.total_visits >= df.total_visits.quantile(0.99)] = df.total_visits.quantile(0.99)\n",
    "df.page_views_per_visit.loc[df.page_views_per_visit >= \n",
    "                            df.page_views_per_visit.quantile(0.99)] = df.page_views_per_visit.quantile(0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 14))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "sns.boxplot(df.total_visits)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.boxplot(df.page_views_per_visit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162f678",
   "metadata": {},
   "source": [
    "As we can see, we were able to significantly reduce the number of outliers by capping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bafea",
   "metadata": {},
   "source": [
    "### Test-Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de051f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "X = df.drop(['lead_number', 'converted'], axis=1)\n",
    "\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting response variable to y\n",
    "y = df['converted']\n",
    "\n",
    "y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd3c8a",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['total_visits','time_on_website','page_views_per_visit']] = scaler.fit_transform(\n",
    "    X_train[['total_visits','time_on_website','page_views_per_visit']])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23898b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the conversion rate\n",
    "conversion = (sum(df['converted'])/len(df['converted'].index))*100\n",
    "conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54489c1d",
   "metadata": {},
   "source": [
    "The conversion rate is 38.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e46a5",
   "metadata": {},
   "source": [
    "### Looking at correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the correlation matrix \n",
    "plt.figure(figsize = (14,10))       \n",
    "sns.heatmap(df.corr(),annot = True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f3daee",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1bcd62",
   "metadata": {},
   "source": [
    "### Model 1: All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0909f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\n",
    "logm1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6dc6c8",
   "metadata": {},
   "source": [
    "### Feature selection using RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfee77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate logistic regression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# initiate rfe\n",
    "rfe = RFE(logreg, n_features_to_select=13)  # running RFE with 13 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11531b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b355fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign columns\n",
    "col = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75ad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what columns were not selected by RFE\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca846117",
   "metadata": {},
   "source": [
    "### Model 2: Assessing the model with statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
